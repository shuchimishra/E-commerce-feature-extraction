{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/E-commerce-feature-extraction/blob/master/Disaster_Tweet_w_Basic_EDA_Cleaning_%26_GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'nlp-getting-started:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F17777%2F869809%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T191624Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1b33a5c91c6bd7da5558cc90b56690ed7eee35bbb779b28f43af307c1c4eb3590e7936990329e0efea8d830bc33e0f62fd3163a3d55b7d8a6cc6e5385c825fa580d0561162e2b1866e24431cf26532fefef2ce80b1d7f522ca7a553d4805dab3dd01b5125e2fa0b16a1e5d09fc8ba0d7069a754b1b17a792a4c7bf500622da7e267f4e5bdd08ccbfc40be4b1d72caba0f81691c2c968f7bed2b37f98c12766ac2abb8fe5d2bf608982e80bcb62b7c365673f3b58b5aa52022025366d1a89de159017b247ed7bd01187ae33fccd6e641589d3e342e9df6127cf2156f709ed4acb14da44b098fb5c56de16188d6d14767855d3fa67cd02febf73109d422a8ff1cd,glove-global-vectors-for-word-representation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1835%2F3176%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T191624Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6bddc53e60bc10941e5f560eec9fe10ec21c736dca52c6807ec4f0f2a89390688081740644d967b3d7c066ef48d6d2de041271014713469acb71853aa33f543ad16bf59c7b49af211f27f07b9ab0e538ad8f4471bb6de96d4c9f49e997e9dcfa6060525030cf52d91b9c47f926189d90e6149fe7939b86217352655bf2e97b6f1f795717978924c2bdef2096317649e9e31a427b38686f95fd3b37172ac097b49eee2e0ffc44ec7457ef0ea29091cfca9747009214696b0174dcd4cbde2efc23fa77753163d170c6d9fe254900c721a02c393a3a202da965ce8044b3976d965513ba5d8be99942a06f164bf73e06525560f90d85c7f9071777bc97442a615b4c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "JhyswT2evV3P"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Rp20UjTmvV3R"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Intro\n",
        "\n",
        "In this competition, youâ€™re challenged to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t.\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)"
      ]
    },
    {
      "metadata": {
        "id": "TdJECp3DvV3S"
      },
      "cell_type": "markdown",
      "source": [
        "## What's in this kernel?\n",
        "- Basic EDA\n",
        "- Data Cleaning\n",
        "- Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "__AnQjWBvV3S"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing required Libraries."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9Hw5VKbmvV3S"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import numpy as np\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.util import ngrams\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from collections import defaultdict\n",
        "# from collections import  Counter\n",
        "# plt.style.use('ggplot')\n",
        "# stop=set(stopwords.words('english'))\n",
        "# import re\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# import gensim\n",
        "# import string\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from tqdm import tqdm\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
        "# from keras.initializers import Constant\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zStcBPJ6vV3S"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.listdir('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Optional] Copy source data files**"
      ],
      "metadata": {
        "id": "I7p4r57VveL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sxCjCi9hvkb2",
        "outputId": "ad5fde4e-7b6d-49fd-b5aa-f10afb817d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp '/content/drive/MyDrive/Data Science & Machine Learning/Tensorflow Certification/Repository/Tensorflow_projects/Data/DisasterTweets dataset/train.csv' './data/'\n",
        "!cp '/content/drive/MyDrive/Data Science & Machine Learning/Tensorflow Certification/Repository/Tensorflow_projects/Data/DisasterTweets dataset/test.csv' './data/'\n",
        "!cp '/content/drive/MyDrive/Data Science & Machine Learning/Tensorflow Certification/Repository/Tensorflow_projects/Data/DisasterTweets dataset/test.csv' './data/'"
      ],
      "metadata": {
        "id": "U7T8t40VvdPT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('./glove.6B.zip', 'r')\n",
        "zip_ref.extractall('./data')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Fv2J9suUwebI",
        "outputId": "00b21f93-dbf4-46fa-a4ce-7b239ee02c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-12 19:22:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-12 19:22:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-12 19:22:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 38s  \n",
            "\n",
            "2024-04-12 19:25:06 (5.19 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYqG0Fp0vV3T"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data and getting basic idea"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tZmtI2BOvV3T",
        "outputId": "e3bbfa1f-907b-42c3-81d2-1a4a6f71be55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "cell_type": "code",
      "source": [
        "tweet= pd.read_csv('./data/train.csv')\n",
        "test=pd.read_csv('./data/test.csv')\n",
        "tweet.head(3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-94f51433d959>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/nlp-getting-started/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/nlp-getting-started/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gWlsT4iLvV3T"
      },
      "cell_type": "code",
      "source": [
        "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
        "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NiO4V_DQvV3T"
      },
      "cell_type": "markdown",
      "source": [
        "## Class distribution"
      ]
    },
    {
      "metadata": {
        "id": "5l4kjXQivV3T"
      },
      "cell_type": "markdown",
      "source": [
        "Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1j7u-dm8vV3U"
      },
      "cell_type": "code",
      "source": [
        "x=tweet.target.value_counts()\n",
        "sns.barplot(x.index,x)\n",
        "plt.gca().set_ylabel('samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuLhM6HtvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)"
      ]
    },
    {
      "metadata": {
        "id": "tfO2p3SWvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis of tweets"
      ]
    },
    {
      "metadata": {
        "id": "dh1AwKFsvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "First,we will do very basic analysis,that is character level,word level and sentence level analysis."
      ]
    },
    {
      "metadata": {
        "id": "XpeCN_ykvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "### Number of characters in tweets"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CRVJrokDvV3U"
      },
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "tweet_len=tweet[tweet['target']==1]['text'].str.len()\n",
        "ax1.hist(tweet_len,color='red')\n",
        "ax1.set_title('disaster tweets')\n",
        "tweet_len=tweet[tweet['target']==0]['text'].str.len()\n",
        "ax2.hist(tweet_len,color='green')\n",
        "ax2.set_title('Not disaster tweets')\n",
        "fig.suptitle('Characters in tweets')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMdU0_UjvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both."
      ]
    },
    {
      "metadata": {
        "id": "J6yDuvaYvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "### Number of words in a tweet"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KhQMS47ovV3U"
      },
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\n",
        "ax1.hist(tweet_len,color='red')\n",
        "ax1.set_title('disaster tweets')\n",
        "tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\n",
        "ax2.hist(tweet_len,color='green')\n",
        "ax2.set_title('Not disaster tweets')\n",
        "fig.suptitle('Words in a tweet')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kmbMWIBKvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "###  Average word length in a tweet"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hqRCmgd0vV3U"
      },
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
        "word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n",
        "ax1.set_title('disaster')\n",
        "word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n",
        "ax2.set_title('Not disaster')\n",
        "fig.suptitle('Average word length in each tweet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FHZT2gaevV3U"
      },
      "cell_type": "code",
      "source": [
        "def create_corpus(target):\n",
        "    corpus=[]\n",
        "\n",
        "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98qo9i10vV3U"
      },
      "cell_type": "markdown",
      "source": [
        "### Common stopwords in tweets"
      ]
    },
    {
      "metadata": {
        "id": "z5JcMEkRvV3U"
      },
      "cell_type": "markdown",
      "source": [
        "First we  will analyze tweets with class 0."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y1T_hXWfvV3U"
      },
      "cell_type": "code",
      "source": [
        "corpus=create_corpus(0)\n",
        "\n",
        "dic=defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop:\n",
        "        dic[word]+=1\n",
        "\n",
        "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Tj3XNfUtvV3V"
      },
      "cell_type": "code",
      "source": [
        "x,y=zip(*top)\n",
        "plt.bar(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKV9TxsRvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "Now,we will analyze tweets with class 1."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YEzctnxEvV3V"
      },
      "cell_type": "code",
      "source": [
        "corpus=create_corpus(1)\n",
        "\n",
        "dic=defaultdict(int)\n",
        "for word in corpus:\n",
        "    if word in stop:\n",
        "        dic[word]+=1\n",
        "\n",
        "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "\n",
        "\n",
        "\n",
        "x,y=zip(*top)\n",
        "plt.bar(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "id": "XF0jbDGrvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "id": "E0WUnlkyvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "### Analyzing punctuations."
      ]
    },
    {
      "metadata": {
        "id": "Yt7BSJazvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "First let's check tweets indicating real disaster."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "weucg50-vV3V"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "corpus=create_corpus(1)\n",
        "\n",
        "dic=defaultdict(int)\n",
        "import string\n",
        "special = string.punctuation\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "\n",
        "x,y=zip(*dic.items())\n",
        "plt.bar(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36kAjkPfvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "Now,we will move on to class 0."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YXlkFy6KvV3V"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "corpus=create_corpus(0)\n",
        "\n",
        "dic=defaultdict(int)\n",
        "import string\n",
        "special = string.punctuation\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "\n",
        "x,y=zip(*dic.items())\n",
        "plt.bar(x,y,color='green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxqbFfQvvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "### Common words ?"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-J1WYSAbvV3V"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "counter=Counter(corpus)\n",
        "most=counter.most_common()\n",
        "x=[]\n",
        "y=[]\n",
        "for word,count in most[:40]:\n",
        "    if (word not in stop) :\n",
        "        x.append(word)\n",
        "        y.append(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ULrA9kECvV3V"
      },
      "cell_type": "code",
      "source": [
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NL6gZCalvV3V"
      },
      "cell_type": "markdown",
      "source": [
        "Lot of cleaning needed !"
      ]
    },
    {
      "metadata": {
        "id": "UJxTfPyOvV3Z"
      },
      "cell_type": "markdown",
      "source": [
        "### Ngram analysis"
      ]
    },
    {
      "metadata": {
        "id": "Y8S4L2RSvV3Z"
      },
      "cell_type": "markdown",
      "source": [
        "we will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eCHT3w-9vV3Z"
      },
      "cell_type": "code",
      "source": [
        "def get_top_tweet_bigrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XYOu-cJXvV3Z"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\n",
        "x,y=map(list,zip(*top_tweet_bigrams))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pDTmp3I8vV3Z"
      },
      "cell_type": "markdown",
      "source": [
        "We will need lot of cleaning here.."
      ]
    },
    {
      "metadata": {
        "id": "OrVxG7cIvV3a"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fE9Tb9fVvV3a"
      },
      "cell_type": "code",
      "source": [
        "df=pd.concat([tweet,test])\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgsnuJ1YvV3a"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing urls"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "c0v8CJPxvV3a"
      },
      "cell_type": "code",
      "source": [
        "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "X6uStA6yvV3a"
      },
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)\n",
        "\n",
        "remove_URL(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XXEXKEdhvV3a"
      },
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(lambda x : remove_URL(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfZuajpXvV3a"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing HTML tags"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3X7AEzITvV3a"
      },
      "cell_type": "code",
      "source": [
        "example = \"\"\"<div>\n",
        "<h1>Real or Fake</h1>\n",
        "<p>Kaggle </p>\n",
        "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
        "</div>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JjXDix-OvV3a"
      },
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "print(remove_html(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Zs8G51I_vV3a"
      },
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(lambda x : remove_html(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3l8oFXUyvV3a"
      },
      "cell_type": "markdown",
      "source": [
        "### Romoving Emojis"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LgMzKtK9vV3a"
      },
      "cell_type": "code",
      "source": [
        "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "remove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nmV4bD6AvV3b"
      },
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(lambda x: remove_emoji(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwnRFOTSvV3b"
      },
      "cell_type": "markdown",
      "source": [
        "### Removing punctuations"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a3MswGjKvV3b"
      },
      "cell_type": "code",
      "source": [
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #king\"\n",
        "print(remove_punct(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ggWHxHLpvV3b"
      },
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(lambda x : remove_punct(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERZlf-0NvV3b"
      },
      "cell_type": "markdown",
      "source": [
        "### Spelling Correction\n"
      ]
    },
    {
      "metadata": {
        "id": "gN21wyttvV3b"
      },
      "cell_type": "markdown",
      "source": [
        "Even if I'm not good at spelling I can correct it with python :) I will use `pyspellcheker` to do that."
      ]
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "TiEMQixovV3b"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MllWGmCCvV3b"
      },
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "text = \"corect me plese\"\n",
        "correct_spellings(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ERaj3yvuvV3b"
      },
      "cell_type": "code",
      "source": [
        "#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUBQDsG1vV3b"
      },
      "cell_type": "markdown",
      "source": [
        "## GloVe for Vectorization"
      ]
    },
    {
      "metadata": {
        "id": "sd6iB5AWvV3b"
      },
      "cell_type": "markdown",
      "source": [
        "Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aHSZWJ8LvV3b"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def create_corpus(df):\n",
        "    corpus=[]\n",
        "    for tweet in tqdm(df['text']):\n",
        "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
        "        corpus.append(words)\n",
        "    return corpus\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-cdARl9-vV3b"
      },
      "cell_type": "code",
      "source": [
        "corpus=create_corpus(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yoGr0fXevV3b"
      },
      "cell_type": "code",
      "source": [
        "embedding_dict={}\n",
        "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "z2mZ7fSmvV3b"
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN=50\n",
        "tokenizer_obj=Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(corpus)\n",
        "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XNU_jDh8vV3b"
      },
      "cell_type": "code",
      "source": [
        "word_index=tokenizer_obj.word_index\n",
        "print('Number of unique words:',len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GB8yg7FqvV3b"
      },
      "cell_type": "code",
      "source": [
        "num_words=len(word_index)+1\n",
        "embedding_matrix=np.zeros((num_words,100))\n",
        "\n",
        "for word,i in tqdm(word_index.items()):\n",
        "    if i > num_words:\n",
        "        continue\n",
        "\n",
        "    emb_vec=embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i]=emb_vec\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KY1JO5povV3b"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tRID4_5TvV3b"
      },
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "\n",
        "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
        "                   input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "model.add(embedding)\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "optimzer=Adam(learning_rate=1e-5)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9lIQNMw3vV3b"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MojdIWZgvV3c"
      },
      "cell_type": "code",
      "source": [
        "train=tweet_pad[:tweet.shape[0]]\n",
        "test=tweet_pad[tweet.shape[0]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PtunosrivV3c"
      },
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
        "print('Shape of train',X_train.shape)\n",
        "print(\"Shape of Validation \",X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jQ5sf_4uvV3c"
      },
      "cell_type": "code",
      "source": [
        "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPwvTcvavV3c"
      },
      "cell_type": "markdown",
      "source": [
        "## Making our submission"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FzJKQ6mnvV3c"
      },
      "cell_type": "code",
      "source": [
        "sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0PB5S-79vV3c"
      },
      "cell_type": "code",
      "source": [
        "y_pre=model.predict(test)\n",
        "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
        "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
        "sub.to_csv('submission.csv',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FRgjmNcHvV3c"
      },
      "cell_type": "code",
      "source": [
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3m9fD-4lvV3c"
      },
      "cell_type": "markdown",
      "source": [
        "<font size='5' color='red'>  if you like this kernel,please do an upvote.</font>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a8ryqGZ4vV3c"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Basic EDA,Cleaning and GloVe",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}